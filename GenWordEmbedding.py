import numpy as np
from LAC import LAC
import time
import re
import MeCab
import spacy
import wordninja
from langdetect import detect
import wordsegment

##### all the files needed
input_descriptions = 'C:\\Users\\linka\\OneDrive\\Desktop\\Code\\HuiFu\\36k1\\36k1-copy.csv'
input_categories = 'C:\\Users\\linka\\OneDrive\\Desktop\\Code\\HuiFu\\36k1\\36k1Res.txt'
vector_endpoint_base_path = 'C:\\Users\linka\\OneDrive\\Desktop\\Code\\HuiFu\\TrainingDataVectorized\\'
cn_vectors_file = vector_endpoint_base_path + 'ChineseVectors.txt'
jp_vectors_file = vector_endpoint_base_path + 'JapaneseVectors.txt'
eng_vectors_file = vector_endpoint_base_path + 'EnglishVectors.txt'
latin_vectors_file = vector_endpoint_base_path + 'LatinVectors.txt'
cn_vectors_endpoints_file = vector_endpoint_base_path + 'ChineseEndpoints.txt'
jp_vectors_endpoints_file = vector_endpoint_base_path + 'JapaneseEndpoints.txt'
eng_vectors_endpoints_file = vector_endpoint_base_path + 'EnglishEndpoints.txt'
latin_vectors_endpoints_file = vector_endpoint_base_path + 'LatinEndpoints.txt'
run_counter_file = 'C:\\Users\linka\\OneDrive\\Desktop\\Code\\HuiFu\\TrainingDataVectorized\\RunCount.txt'

vector_file_mappings = {
    1: (jp_vectors_file, jp_vectors_endpoints_file),
    2: (cn_vectors_file, cn_vectors_endpoints_file),
    0: (eng_vectors_file, eng_vectors_endpoints_file),
    3: (latin_vectors_file, latin_vectors_endpoints_file)
}
# the one hot endpoints
one_hot_categories = {
    "clothing/bags": [1, 0, 0, 0, 0, 0],
    "medicine/food": [0, 1, 0, 0, 0, 0],
    "makeup": [0, 0, 1, 0, 0, 0],
    "electronics": [0, 0, 0, 1, 0, 0],
    "furniture": [0, 0, 0, 0, 1, 0],
    "others": [0, 0, 0, 0, 0, 1]
}
# count the amount of languages we scanned
language_counts = {0: 0, 1: 0, 2: 0, 3:0, 4:0, 5:0, -1:0}
# for detecting language
language_mapping_lang_detect = {
    'en': 0,
    'ja': 1,
    'zh-cn': 2,
    'es': 3,
    'it': 3,
    'fr': 4,
    'de': 5
 }

# Initialize Baidu LAC
lac = LAC(mode='rank')
#initialize wordsegment for Spanish tokenization
wordsegment.load()
# this is for generalizing the vectors generated by 3 different word embedding files
def z_score_normalization(embedding):
    mean = np.mean(embedding)
    std_dev = np.std(embedding)
    standardized_embedding = (embedding - mean) / std_dev
    return standardized_embedding

def remove_non_alphabet_characters(input_string):
    # Use a regular expression to remove all non-alphabet characters
    cleaned_string = re.sub(r'[^a-zA-Z ]', '', input_string)
    return cleaned_string

def load_word_vectors(file_path):
    print("loading word embedding vectors")
    print(file_path)
    start_time = time.time()
    # Initialize an empty dictionary to store word vectors
    word_vectors = {}
    # Open and read the Tencent word embedding text file
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            # Split each line into word and vector parts
            parts = line.strip().split(' ')
            word = parts[0]
            vector = np.array([float(val) for val in parts[1:]], dtype='float32')

            # Store the word vector in the dictionary
            word_vectors[word] = vector
    end_time = time.time()
    execution_time = (end_time - start_time)
    print(f"Loading time: {execution_time:.5f} seconds")
    return word_vectors

#def Vector_decomposition(embedding_300d):
    # Create a PCA model with 200 components
    #pca = PCA(n_components=200)

    # Fit the PCA model to your 300-dimensional embeddings
    #pca.fit(embedding_300d)

    # Transform your 300-dimensional embeddings into a 200-dimensional space
    #embeddings_200d = pca.transform(embedding_300d)
    #print ("done compressing")
    #return embeddings_200d

##### below are token splitters
def split_tokens_cn_lac(text):
    results = lac.run(text)
    tokens = results[0]
    return tokens

def split_tokens_eng_wordninja(text):
    tokens = wordninja.split(text)
    return tokens

spacy_nlp_eng = spacy.load("en_core_web_sm")
def split_tokens_eng_spacy(text):
    return split_tokens_spacy(text, spacy_nlp_eng)

spacy_nlp_es = spacy.load("es_core_news_sm")
def split_tokens_es_spacy(text):
    return split_tokens_spacy(text,spacy_nlp_es)

def split_tokens_es_word_segment(text):
    return wordsegment.segment(text)

def split_tokens_spacy(text , spacy_nlp):
    text = remove_non_alphabet_characters(text)
    nlp_res = spacy_nlp(text)
    tokens = [token.text for token in nlp_res]
    return tokens

def split_tokens_jp_mecab(text):
    m = MeCab.Tagger()
    node = m.parse(text)
    tokens = [line.split('\t')[0] for line in node.split('\n') if line]
    return tokens

def correct_segmented_words(segmented_words, text):
    text = text.lower()
    orig_itr = 0
    orig_len = len(text)
    seg_itr = 0
    seg_w_itr = 0
    
    words = []
    cur_word = ""
    
    while orig_itr < orig_len:
        s = segmented_words[seg_itr][seg_w_itr]
        t = text[orig_itr]
        if s == t:
            cur_word += t
            seg_w_itr += 1
            
            if seg_w_itr == len(segmented_words[seg_itr]):  # move to the next word
                seg_itr += 1
                seg_w_itr = 0
                words.append(cur_word)
                cur_word = ""
        else:
            cur_word += t  # Use the character from text
            
        orig_itr += 1

    # Append the last word if any
    if cur_word:
        words.append(cur_word)

    return words

def split_tokens_french_custom(text):
    segmented_words = wordsegment.segment(text)
    tokens = correct_segmented_words(segmented_words, text)
    return tokens

def split_tokens_german_custom(text):
    segmented_words = wordsegment.segment(text)
    tokens = correct_segmented_words(segmented_words, text)
    return tokens
########## end of token splitters

def calculate_text_vector(tokens, word_vectors):
    start_time = time.time()
    # if cant find, we do 200 0s, or else we add it on there and average
    text_vector = np.mean([word_vectors.get(word.lower(), np.zeros(200)) for word in tokens], axis=0)
    end_time = time.time()
    execution_time = (end_time - start_time)
    print(f"Vector Generation Execution time: {execution_time:.5f} seconds")
    return text_vector

# hiragana and katakana = jp, kanji might be chinese
def is_hiragana_or_katakana(char):
    code_point = ord(char)
    return (0x3040 <= code_point <= 0x309F) or (0x30A0 <= code_point <= 0x30FF)

# traditional or simplified chinese = chinese letter, might be jp tho
def is_simplified_or_traditional_chinese(char):
    code_point = ord(char)
    # Range for Simplified Chinese (common): U+4E00 to U+9FFF
    simplified_range = (0x4E00 <= code_point <= 0x9FFF)
    # Range for Traditional Chinese (common): U+3400 to U+4DBF and U+20000 to U+2A6DF
    traditional_range = (0x3400 <= code_point <= 0x4DBF) or (0x20000 <= code_point <= 0x2A6DF)
    return simplified_range or traditional_range

# 0 eng
# 1 jp
# 2 cn
# 3 es
# 3 it
# italian and spanish have the same root, which is latin
# -1 other language
def determine_langauge(text):
    lang = 0
    # chinese might have weird eng in it
    for char in text:
        if(is_hiragana_or_katakana(char)):
            return 1
        if(is_simplified_or_traditional_chinese(char)):
            lang = 2
    if(lang != 1 and lang != 2):
        # lang detect might fail due to no language given

        
        lang2 = detect(text)
        
        print(f"lang detect detected language: {lang2}")
        lang = language_mapping_lang_detect.get(lang2, -1)
    return lang

# rid numbers
def clean_text1(text):
    text = text.strip()
    # Create a translation table to remove numbers and non-word characters
    translation_table = str.maketrans('', '', '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~')
    #translation_table = str.maketrans('', '', '1234567890!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~')
    # Apply the translation to the text
    cleaned_text = text.translate(translation_table)
    cleaned_text = re.sub(r'[^\w\sぁ-んァ-ヶ一-龯]', '', cleaned_text)
    return cleaned_text

# dont rid numbers
def clean_text2(text):
    text = text.strip()
    # Create a translation table to remove numbers and non-word characters
    #translation_table = str.maketrans('', '', '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~')
    translation_table = str.maketrans('', '', '1234567890!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~')
    # Apply the translation to the text
    cleaned_text = text.translate(translation_table)
    cleaned_text = re.sub(r'[^\w\sぁ-んァ-ヶ一-龯]', '', cleaned_text)
    return cleaned_text

# returns non z scored result
def process_text2_p1(text, language_code, language_mappings):
    word_vectors = np.zeros(200)
    lang = language_code
    #lang2 = detect(text)
    #print(f"lang detect detected language: {lang2}")
    # split tokens, generate word embeddings
    start_time = time.time()
    if lang in language_mappings:
        lang_info = language_mappings[lang]
        print(lang_info[0])
        tokens = lang_info[1](text)
        # rid all empty tokens
        tokens = [token for token in tokens if token.strip()]
        word_vectors = calculate_text_vector(tokens, lang_info[2])
        print("these are the tokens:")
        for token in tokens:
            print(token)
    else:
        print("Unsupported language")
    end_time = time.time()
    execution_time = end_time - start_time
    print("pre zscore:")
    print(word_vectors)
    print(f"Word embedding Overall Execution time: {execution_time:.5f} seconds")
    return word_vectors

def check_valid_vector(word_vector):
    # Check if all elements are either 0 or NaN
    contains_only_zeros_or_nans = np.all(np.logical_or(np.isnan(word_vector), word_vector == 0))
    return not(contains_only_zeros_or_nans)

def process_text2(text, language_code):
    # dont rid numbers
    text = clean_text1(text)
    word_vectors = process_text2_p1(text, language_code)
    if not(check_valid_vector(word_vectors)):
        # rid numbers
        text = clean_text2(text)
        word_vectors = process_text2_p1(text, language_code)
    if not(check_valid_vector(word_vectors)):
        return -1, word_vectors
    return 1, z_score_normalization(word_vectors)

def process_text(text, language_mappings):
    # first determine what kind of language the text is 
    # get rid of weird stuff
    word_vectors = np.zeros(200)
    text = clean_text1(text)
    lang = determine_langauge(text)
    #lang2 = detect(text)
    #print(f"lang detect detected language: {lang2}")
    # split tokens, generate word embeddings
    start_time = time.time()
    if lang in language_mappings:
        lang_info = language_mappings[lang]
        print(lang_info[0])
        tokens = lang_info[1](text)
        # rid all empty tokens
        tokens = [token for token in tokens if token.strip()]
        word_vectors = calculate_text_vector(tokens, lang_info[2])
        for token in tokens:
            print(token)
    else:
        print("Unsupported language")
    end_time = time.time()
    execution_time = end_time - start_time
    print("pre zscore:")
    print(word_vectors)
    print(f"Word embedding Overall Execution time: {execution_time:.5f} seconds")
    return z_score_normalization(word_vectors)


def remove_last_line_from_file(file_path):
    try:
        # Read the content of the file
        with open(file_path, "r") as file:
            lines = file.readlines()

        # Check if there are lines to remove
        if lines:
            # Remove the last line
            updated_lines = lines[:-1]

            # Write the updated content back to the file
            with open(file_path, "w") as file:
                file.writelines(updated_lines)

        print("Last line removed from the file.")
    except FileNotFoundError:
        print("File not found.")
    except Exception as e:
        print(f"An error occurred: {e}")

def remove_all_last_lines(vector_file_mappings):
    for vector_type, (vector_file, endpoints_file) in vector_file_mappings.items():
        remove_last_line_from_file(vector_file)
        remove_last_line_from_file(endpoints_file)

def print_language_done():
    for code, count in language_counts.items():
        if code == -1:
            print(f"Language Code {code} (Tokenize Failed): {count}")
        elif code in language_mapping_lang_detect.values():
            language_codes = [key for key, value in language_mapping_lang_detect.items() if value == code]
            for language_code in language_codes:
                print(f"Language Code {code} ({language_code}): {count}")
    total_count = sum(language_counts.values())
    print("Total Count:", total_count)  

def Vectorize_results(language_mapping):
    counter = 0
    with open(input_descriptions, "r", encoding="utf-8") as in_des, open(input_categories, "r", encoding="utf-8") as in_cat:
        for description_line, category_line in zip(in_des, in_cat):
            counter += 1
            print("The current item is (non 0)" + str(counter))
            # Process description_line and category_line together
            lang = -1
            try:
                lang = determine_langauge(description_line)
            except:
                print("unable to determine language")
                language_counts[-1] += 1
                pass
            if lang == 0 or lang == 1 or lang == 2 or lang == 3:
                success, res = process_text2(description_line, lang, language_mapping)
                # res returns -1 if we cant tokenize for some reason even though language was recognized
                if(success == -1):
                    print("contains nan or np 0s, unable to tokenize text, skipping")
                    language_counts[-1] += 1
                    continue
                if(not category_line.strip()): # no results given in the previous step when asking chatgpt
                    print("no result given. skipping")
                    language_counts [-1] += 1
                    continue
                cat = one_hot_categories[category_line.strip()]
                with open(vector_file_mappings[lang][0], "a") as v, open(vector_file_mappings[lang][1], "a") as e:
                    print("writing")
                    language_counts[lang] += 1
                    vector_str = ' '.join(str(value) for value in res)
                    v.write(vector_str + '\n')
                    e.write(str(cat) + '\n')
            #time.sleep(2)
            else:
                print("unsupported language")
    #remove_all_last_lines() # last lines are gonna be empty
    print_language_done()
    with open(run_counter_file, "r+") as file:
        lines = file.readlines()  # Read all lines into a list
        # Go back to the beginning of the file
        file.seek(0)

        # Iterate over lines and language counts and write the modified lines
        for line, count in zip(lines, language_counts.values()):
            modified_line = line.strip() + f", {count}\n"
            file.write(modified_line)

        # Truncate the file in case the new content is shorter than the original content
        file.truncate()
        




pretrained_base_path = 'C:\\Users\\linka\\OneDrive\\Desktop\\Code\\HuiFu\\WordEmbedding\\WordEmbeddingSources\\'
# Load Tencent word vectors
tencent_word_vectors = load_word_vectors(pretrained_base_path + 'tencentAi200\\tencentAi200\\tencentAiEmbedding200.txt')
# Load Glove200 word vectors
glove200_word_vectors = load_word_vectors(pretrained_base_path + 'glove.6B\\glove.6B.200d.txt')
# Load DepJa200 word vectors
depja200_word_vectors = load_word_vectors(pretrained_base_path + 'dep-ja-200dim.txt')
# Load Spanish word vectors
spanish300_word_vectors = load_word_vectors(pretrained_base_path + "SBW-vectors-300-min5.txt")
#spanish200_word_vectors = Vector_decomposition(spanish300_word_vectors)
language_mappings = {
        1: ("jp sentence", split_tokens_jp_mecab, depja200_word_vectors),
        2: ("cn sentence", split_tokens_cn_lac, tencent_word_vectors),
        0: ("eng sentence", split_tokens_eng_wordninja, glove200_word_vectors),
        3: ("latin sentence", split_tokens_es_word_segment, spanish300_word_vectors)
    }


Vectorize_results(language_mappings)
