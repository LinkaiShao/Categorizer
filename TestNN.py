import tensorflow as tf
from tensorflow.keras.models import load_model
import numpy as np
import wordninja
import time
model = load_model("C:\\Users\\linka\\OneDrive\\Desktop\\Code\\HuiFu\\WordEmbedding\\TrainedNN\\double_dropout300.h5")
pretrained_base_path = 'D:\\WordEmbedding300'
cn300_embedding_path = "D:\\WordEmbedding300\\cc.zh.300.vec\\cc.zh.300.vec"
jp300_embedding_path = "D:\\WordEmbedding300\\cc.ja.300.vec\\cc.ja.300.vec"
en300_embedding_path = "D:\\WordEmbedding300\\cc.en.300.vec\\cc.en.300.vec"
es300_embedding_path = "D:\\WordEmbedding300\\cc.es.300.vec\\cc.es.300.vec"
fr300_embedding_path = "D:\\WordEmbedding300\\cc.fr.300.vec\\cc.fr.300.vec"
de300_embedding_Path = "D:\\WordEmbedding300\\cc.de.300.vec\\cc.de.300.vec"
one_hot_categories = {
    "clothing/bags": [1, 0, 0, 0, 0, 0],
    "medicine/food": [0, 1, 0, 0, 0, 0],
    "makeup": [0, 0, 1, 0, 0, 0],
    "electronics": [0, 0, 0, 1, 0, 0],
    "furniture": [0, 0, 0, 0, 1, 0],
    "others": [0, 0, 0, 0, 0, 1]
}
def load_word_vectors(file_path):
    print("loading word embedding vectors")
    print(file_path)
    start_time = time.time()
    # Initialize an empty dictionary to store word vectors
    word_vectors = {}
    # Open and read the Tencent word embedding text file
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            # Split each line into word and vector parts
            parts = line.strip().split(' ')
            word = parts[0]
            vector = np.array([float(val) for val in parts[1:]], dtype='float32')

            # Store the word vector in the dictionary
            word_vectors[word] = vector
    end_time = time.time()
    execution_time = (end_time - start_time)
    print(f"Loading time: {execution_time:.5f} seconds")
    return word_vectors

def calculate_text_vector(tokens, word_vectors):
    start_time = time.time()
    # if cant find, we do 200 0s, or else we add it on there and average
    text_vector = np.mean([word_vectors.get(word.lower(), np.zeros(300)) for word in tokens], axis=0)
    end_time = time.time()
    execution_time = (end_time - start_time)
    print(f"Vector Generation Execution time: {execution_time:.5f} seconds")
    return text_vector

# this is for generalizing the vectors generated by 3 different word embedding files
def z_score_normalization(embedding):
    mean = np.mean(embedding)
    std_dev = np.std(embedding)
    standardized_embedding = (embedding - mean) / std_dev
    return standardized_embedding
def split_tokens_eng_wordninja(text):
    tokens = wordninja.split(text)
    return tokens

def predict_category(model, input_vector, category_mapping):
    # Reshape the input_vector to match the model's input shape
    input_vector = np.array(input_vector).reshape(1, -1)
    
    # Get the model's prediction
    prediction = model.predict(input_vector)
    print(prediction)
    # Get the index of the maximum probability
    predicted_index = np.argmax(prediction, axis=1)[0]
    
    # Convert the index to word
    predicted_category = list(category_mapping.keys())[predicted_index]
    
    return predicted_category

tokens = split_tokens_eng_wordninja("Steak Sauce")
word_vectors = load_word_vectors(en300_embedding_path)
text_vector = calculate_text_vector(tokens, word_vectors)
zscored_vector = z_score_normalization(text_vector)
zscored_vector = zscored_vector.reshape(1,-1)
print(zscored_vector)
final_cat = predict_category(model,zscored_vector,one_hot_categories)
print(final_cat)